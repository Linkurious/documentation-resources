# User-Data Store Migration Tool

## Context
Linkurious Enterprise is shipped with an embedded Sqlite database
to allow an immediate usage of the system in a PoC environment, however it's a best practice to relay on a relational databased management system (RDBMS) in a production environment.

It's a common need of our customers to migrate the data from the Sqlite database to an external relational database.

Since different databases may have different peculiarities, this tools is aimed to provide the most reliable process to safely migrate the data.

> /!\ The tool's objective is to migrate data from Sqlite to an external db for the same Linkurious Enterprise instance. No other scenario are covered.

# The migration process

Linkurious Enterprise is designed to create and maintain (through the updates) it's data structure over the supported databases.

The safest migration approach is to let the system handle the whole task of defining the db schema and then perform manual tasks only to reimport the data.

The tool is designed to help a database administrator to generate the correct script to import data staring from a dump. In particular the script will:
1. remove all the `DDL` instructions;
2. change all the `insert` instructions to explicitly add the field name (to ensure the correct mapping of data regardless possible difference in the order);
3. `Truncate` all the table's contents before data insert (it's needed to delete any default data generated by the system during the schema creation);
4. correct any known compatibility issue between the output syntax of sqlite3 dump and the syntax used by the supported databases;
5. list in output all the `select` instructions needed to access all the migrated data, it could be useful for running any query to verify the correctness of the migration.


The full list of steps would be:
1. Create a text dump of the current Sqlite file with the sqlite3 tool:
   ```shell
   sqlite3 ./linkurious/data/server/database.sqlite .dump > export.sql
   ```
2. Put the `export.sql` file in the same directory of the `dump-converter.py` python script
3. Run the program to generate a new SQL import script (example for MySQL):
   ```shell
   python3 dump-converter.py --dialect mysql export.sql > select-queries.sql
   ```
4. The program will generate two files:
   - export-parsed.sql : the new SQL import file to be used instead of the initial one
   - select-queries.sql : a script containing the `select` queries for all the tables found in the initial script. It can be useful for debugging / crosscheck purposes.
5. Setup your external database server and create an empty database
6. Configure Linkurious Enterprise to point the new Database (refer to the correct version of the [documentation](https://doc.linkurio.us) in case of doubts)
7. Start Linkurious Enterprise
8. As soon as it starts properly (you can see the application, regardless the connection error to the graph database), stop it.
9. Run the new script in your system to import data (example for MySQL):
   ```shell
   mysql -u MY_MYSQL_USER -p -h 127.0.0.1 linkurious < export-parsed.sql
   ```
10. After the import script is executed successfully you can start again Linkurious Enterprise and accessing all the previous data


# How to use / customize the tool

The script is develop to work without any change based on the current known issues at the time of the development. Few parameters are possible to configure the destination database or the input / output file, below the extract of the help.

```shell
$ python3 dump-converter.py -h
usage: python3 dump-converter.py [options] INPUT > select-queries.sql

Linkurious Enterprise User-Data Store dump converter

positional arguments:
  INPUT                 the Sqlite dump to convert (a *.sql file)

optional arguments:
  -h, --help            show this help message and exit
  -o OUTPUT, --out OUTPUT
                        the output file for the new import instructions
                        (default: export-parsed.sql)
  --dialect {mysql,mariadb,mssql}
                        the dialect of the destination database (default:
                        mysql)
```

However it's possible to quickly modify the tool to change / add conversion rules to adapt it to your needs.

The tool relied on the below global variables to create the output file:
- `pre`: an array of strings (instructions) to add at the beginning of the output file.
- `replace_rules`: an array of `conversion rules` to apply on each input line. The rules are tried in order of definition. Each rule is a record of 6 elements:
  - A `boolean` indicating whether the second parameter is a regex expression or not (just a single replace).
  - A `string` with the `match rule`. In case or regex, it's possible to define groups to be used in the replace string.
  - A `string` with the `replace string`. In case or regex, it's possible to define placeholders to be replaced with value matched in the groups (`{i}` where `i` is the group index).
  - A `number` with the number of defined groups that will be replace in the `replace string`
  - A function (used only with regex rules) getting as parameters the `regex match` and the current `replace string`. It has to return a `string` with the new `replace string`.
  - A `boolean` indicating whether the system should stop or not to test the other rules when the current rule is matched.
- `post`: an array of strings (instructions) to add at the end of the output file.
